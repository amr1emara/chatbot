{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ea270b-0c8c-4e09-8cec-9f2dc57debeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ## interaction with operating system\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf ##function that build models\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2580b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='./dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "112d88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []  \n",
    "target_texts = [] \n",
    "\n",
    "with open(data_path) as f:  # Opens the file located at data_path to read\n",
    "    lines = f.read().split('\\n')  # Reads the entire file and splits it into lines\n",
    "\n",
    "for line in lines[: min(600, len(lines) - 1)]:  # Iterates through the first 600 lines or all lines except the last one\n",
    "    split_line = line.split('\\t')  # Splits the line into parts using the tab  as a splitter\n",
    "    if len(split_line) >= 2:  # Checks if the line contains at least two words to avoid index errors\n",
    "        input_text = split_line[0]  # Extracts the first part(question) as the input text\n",
    "        target_text = split_line[1]  # Extracts the second part(answr) as the target text\n",
    "        input_texts.append(input_text)  # Appends(add) the input text to the list of input texts\n",
    "        target_texts.append(target_text)  # Appends(add) the target text to the list of target texts\n",
    "    else:\n",
    "        print(f\"Skipping malformed line: {line}\")  # warning for lines that don't have at least two words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('type of input_text', type(input_text))  \n",
    "#input_text is a string because it represents one input line\n",
    "\n",
    "print('type of target_texts', type(target_texts))  \n",
    "#target_texts is a list because it collects multiple target lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lists `input_texts` and `target_texts` into a Pandas 2d DataFrame\n",
    "zippedList = list(zip(input_texts, target_texts))  \n",
    "# Combines the elements of `input_texts` and `target_texts` into pairs (tuples) to create a list of tuples\n",
    "\n",
    "lines = pd.DataFrame(zippedList, columns=['input', 'output'])  \n",
    "# Creates a 2d DataFrame with the paired data, adding column names 'input' and 'output' to represent the input and target text\n",
    "\n",
    "lines.head()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0856c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42da67d",
   "metadata": {},
   "source": [
    "## Preparing input data for the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lines = list()  \n",
    "# Initializes an empty list to store the input text lines\n",
    "\n",
    "for line in lines.input:  \n",
    "    input_lines.append(line)  \n",
    "    # Iterates through the 'input' column of the DataFrame and add each line to input_lines\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()  \n",
    "# Creates a tokenizer  to convert text into numerical values\n",
    "\n",
    "tokenizer.fit_on_texts(input_lines)  \n",
    "#fits the tokenizer on the input text, creating a list of all unique words and assigning each word a number\n",
    "\n",
    "tokenized_input_lines = tokenizer.texts_to_sequences(input_lines)  \n",
    "# Converts each text line into a numeric values based on the tokenizer's word index\n",
    "\n",
    "length_list = list()  \n",
    "# Initializes a list to store the lengths of each tokenized sequence\n",
    "\n",
    "for token_seq in tokenized_input_lines:  \n",
    "    length_list.append(len(token_seq))  \n",
    "    # adds the length of each tokenized sequence to `length_list`\n",
    "\n",
    "max_input_length = np.array(length_list).max()  \n",
    "# Finds the maximum sequence length from the list of lengths\n",
    "\n",
    "print('Input max length is {}'.format(max_input_length))  \n",
    "# Prints the maximum length of the input sequences\n",
    "\n",
    "padded_input_lines = preprocessing.sequence.pad_sequences(tokenized_input_lines, maxlen=max_input_length, padding='post')  \n",
    "# Pads the tokenized input sequences to ensure all sequences are of the same length (using post-padding)\n",
    "\n",
    "encoder_input_data = np.array(padded_input_lines)  \n",
    "# Converts the padded sequences into a Numpy array\n",
    "\n",
    "print('Encoder input data shape -> {}'.format(encoder_input_data.shape))  \n",
    "# Prints the shape of the encoder input data to verify its dimensions\n",
    "\n",
    "input_word_dict = tokenizer.word_index  \n",
    "# Retrieves the word-to-index mapping created by the tokenizer\n",
    "\n",
    "num_input_tokens = len(input_word_dict) + 1  \n",
    "# (index zero is reserved for a purpose ex padding)Calculates the total number of unique tokens, adding 1 to account for zero-based indexing\n",
    "\n",
    "print('Number of Input tokens = {}'.format(num_input_tokens))  \n",
    "# Prints the total number of unique tokens in the input text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadaff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f8a4a",
   "metadata": {},
   "source": [
    "## Preparing input data for the Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lines = list()  \n",
    "# Initializes an empty list to store the modified output text lines\n",
    "\n",
    "for line in lines.output:  \n",
    "    output_lines.append('<START> ' + line + ' <END>')  \n",
    "    #<START> token as the initial input. the decoder generates tokens step-by-step until the <END> token is predicted\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()  \n",
    "# Creates a tokenizer object to convert the output text into numerical values\n",
    "\n",
    "tokenizer.fit_on_texts(output_lines)  \n",
    "# Fits the tokenizer on the output text lines and creates vocab then adding an integer index to each word\n",
    " \n",
    "tokenized_output_lines = tokenizer.texts_to_sequences(output_lines)  \n",
    "# Converts each output text line into a sequence of numeric numbers\n",
    "\n",
    "length_list = list()  \n",
    "# Initializes a list to store the lengths of each tokenized output sequence\n",
    "\n",
    "for token_seq in tokenized_output_lines:  \n",
    "    length_list.append(len(token_seq))  \n",
    "    # adds the length of each tokenized sequence to length_list\n",
    "\n",
    "max_output_length = np.array(length_list).max()  \n",
    "# gets the maximum sequence length from the list of lengths\n",
    "\n",
    "print('Output max length is {}'.format(max_output_length))  \n",
    "\n",
    "padded_output_lines = preprocessing.sequence.pad_sequences(tokenized_output_lines, maxlen=max_output_length, padding='post')  \n",
    "# Pads the tokenized output sequences to make all the sequences are the same length (using post-padding)\n",
    "\n",
    "decoder_input_data = np.array(padded_output_lines)  \n",
    "# Converts the padded sequences into a NumPy array\n",
    "\n",
    "print('Decoder input data shape -> {}'.format(decoder_input_data.shape))  \n",
    "# Prints the shape of the decoder input data to verify its dimensions\n",
    "\n",
    "output_word_dict = tokenizer.word_index  \n",
    "# gets the word-to-index mapping created by the tokenizer for the output text\n",
    "\n",
    "num_output_tokens = len(output_word_dict) + 1  \n",
    "# Calculates the total number of unique tokens in the output text, adding 1 to account for zero-based indexing\n",
    "\n",
    "print('Number of Output tokens = {}'.format(num_output_tokens))  \n",
    "# Prints the total number of unique tokens in the output text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552e2a7",
   "metadata": {},
   "source": [
    "## Preparing target data for the Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4beca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data = list()  \n",
    "# Initializes an empty list to store the target data for the decoder\n",
    "\n",
    "for token_seq in tokenized_output_lines:  \n",
    "    decoder_target_data.append(token_seq[1:])  \n",
    "   # creates the decoder target data by removing the first token (<START>) from each tokenized output sequence\n",
    "    # The target sequence starts from the second token as the model predicts the next word at each step\n",
    "\n",
    "padded_output_lines = preprocessing.sequence.pad_sequences(decoder_target_data, maxlen=max_output_length, padding='post')  \n",
    "# Pads the target sequences to ensure they are the same length as max_output_length (using post-padding)\n",
    "\n",
    "onehot_output_lines = utils.to_categorical(padded_output_lines, num_output_tokens)  \n",
    "# Converts the padded target sequences into one-hot encoded format, where each word is represented as a vector\n",
    "# with a 1 in the position corresponding to the word's index and 0 elsewhere.\n",
    "# The `num_output_tokens' tells the size of the one-hot vector.\n",
    "\n",
    "decoder_target_data = np.array(onehot_output_lines)  \n",
    "# Converts the one-hot encoded target data into a NumPy array.\n",
    "\n",
    "print('Decoder target data shape -> {}'.format(decoder_target_data.shape))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34651147",
   "metadata": {},
   "source": [
    "## Defining the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Inputs\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None,))  \n",
    "# Defines the input layer for the encoder where the input shape is a sequence of variable length (None ashan yt3amel maa kza length)\n",
    "\n",
    "# Encoder Embedding\n",
    "encoder_embedding = tf.keras.layers.Embedding(num_input_tokens, 256, mask_zero=True)(encoder_inputs)  \n",
    "# Embedding layer maps input tokens (num_input_tokens) into dense 256-dimensional(embedding dim) vectors\n",
    "# mask_zero=True allows the model to ignore padding tokens during training\n",
    "\n",
    "# Encoder LSTM\n",
    "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(\n",
    "    256, return_state=True, recurrent_dropout=0.2, dropout=0.2\n",
    ")(encoder_embedding)\n",
    " \n",
    "# LSTM layer processes the embedding sequences and outputs the hidden state and the cell state\n",
    "# recurrent_dropout=0.2 adds dropout to the recurrent connections, and dropout=0.2, 20% of the input features are randomly set to zero during training\n",
    "# return_state=True ensures the final hidden and cell states are returned\n",
    "\n",
    "encoder_states = [state_h, state_c]  \n",
    "# Stores the encoder's hidden and cell states for initializing the decoder\n",
    "\n",
    "# Decoder Inputs\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,))  \n",
    "# Defines the input layer for the decoder, where the input shape is a sequence of variable length (None)\n",
    "\n",
    "# Decoder Embedding\n",
    "decoder_embedding = tf.keras.layers.Embedding(num_output_tokens, 256, mask_zero=True)(decoder_inputs)  \n",
    "# Embedding layer maps output tokens (num_output_tokens) into dense 256-dimensional vectors.\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = tf.keras.layers.LSTM(\n",
    "    256, return_state=True, return_sequences=True, recurrent_dropout=0.2, dropout=0.2\n",
    ")  \n",
    "# LSTM layer processes the decoder's embedding sequences.\n",
    "# return_sequences=True ensures the LSTM outputs a sequence for each time step\n",
    "# return_state=True ensures the final hidden and cell states are returned\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)    \n",
    "# The decoder LSTM is initialized with the encoder's final hidden and cell states\n",
    "\n",
    "# Dense Layer\n",
    "decoder_dense = tf.keras.layers.Dense(num_output_tokens, activation=tf.keras.activations.softmax)  \n",
    "# Fully connected layer applies a softmax activation to predict the next token for each time step\n",
    "\n",
    "output = decoder_dense(decoder_outputs)  \n",
    "# Applies the dense layer to the decoder outputs, producing the final probabilities for each token\n",
    "\n",
    "# Define the Model\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)  \n",
    "# Creates the sequence-to-sequence model with the encoder and decoder as inputs and the token probabilities as output\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')  \n",
    "# Compiles the model using the Adam optimizer(adaptive learning and mostly used) and used for multi-class prediction.\n",
    "\n",
    "# Model Summary\n",
    "model.summary()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ddc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],  # Input data for the encoder and decoder\n",
    "    decoder_target_data,  # Target output for the decoder, one-hot encoded\n",
    "    batch_size=124,  # number of samples per batch for gradient updates\n",
    "    epochs=500# number of epochs\n",
    ")\n",
    "# Trains the model using the provided data, updating weights to minimize the loss function\n",
    "# The `history` object stores training metrics, such as loss, for each epoch\n",
    "\n",
    "model.save('model.h5')  \n",
    "# Saves the trained model to a file named 'model.h5'\n",
    "# The saved model includes the architecture, weights, and optimizer state, allowing it to be reloaded when used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcfbe8",
   "metadata": {},
   "source": [
    "## Inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cca27ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    # Create the encoder model for inference\n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    # Extracts the input layer and outputs the encoder states (hidden and cell states)\n",
    "\n",
    "    # Create input layers for the decoder's hidden and cell states during inference\n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=(256,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=(256,))\n",
    "    # These inputs correspond to the initial states passed to the decoder during inference\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    # Groups the hidden and cell states into a list for easier handling\n",
    "\n",
    "    # Use the existing decoder LSTM layer, but with the new initial states\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding, initial_state=decoder_states_inputs\n",
    "    )\n",
    "    # Processes the decoder embedding with the provided states and returns the next output and states\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    # Groups the new hidden and cell states into a list\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    # Applies the dense (softmax) layer to the LSTM output to generate token probabilities\n",
    "\n",
    "    # Define the decoder model for inference\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,  # Inputs tokens and previous states\n",
    "        [decoder_outputs] + decoder_states  # Outputs predicted tokens and updated states\n",
    "    )\n",
    "\n",
    "    # Return the inference models\n",
    "    return encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5264e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens(sentence: str):\n",
    "   \n",
    "    words = sentence.lower().split()  \n",
    "    # Converts the input sentence to lwercase and splits it into words\n",
    "\n",
    "    tokens_list = list()  \n",
    "    # Initializes an empty list to store the tokenized words\n",
    "\n",
    "    for word in words:  \n",
    "        tokens_list.append(input_word_dict[word])  \n",
    "        # Converts each word to its corresponding integer token using the input word dictionary\n",
    "\n",
    "    return preprocessing.sequence.pad_sequences(\n",
    "        [tokens_list], maxlen=max_input_length, padding='post'\n",
    "    )  \n",
    "    # Pads the tokenized sequence to the `max_input_length` with zeros at the end, make sure same sequence length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inference models\n",
    "enc_model, dec_model = make_inference_models()\n",
    "\n",
    "# Interactive loop for sequence-to-sequence translation\n",
    "for epoch in range(encoder_input_data.shape[0]):\n",
    "    # Encode user input\n",
    "    states_values = enc_model.predict(str_to_tokens(input('User: ')), verbose=False)\n",
    "    \n",
    "    # Initialize target sequence with the <START> token\n",
    "    empty_target_seq = np.zeros((1, 1))\n",
    "    empty_target_seq[0, 0] = output_word_dict['start']\n",
    "    \n",
    "    stop_condition = False  # Flag to indicate when to stop decoding\n",
    "    decoded_translation = ''  # Initialize the translation string\n",
    "    \n",
    "    while not stop_condition:\n",
    "        # Predict the next token and states using the decoder\n",
    "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values, verbose=False)\n",
    "        \n",
    "        # Get the token with the highest probability (argmax)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        \n",
    "        # Convert token index to word\n",
    "        sampled_word = None\n",
    "        for word, index in output_word_dict.items():\n",
    "            if sampled_word_index == index:\n",
    "                decoded_translation += ' {}'.format(word)  # Append the word to the translation\n",
    "                sampled_word = word\n",
    "        \n",
    "        # Stop if the <END> token is generated or the translation exceeds max length\n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence for the next time step\n",
    "        empty_target_seq = np.zeros((1, 1))\n",
    "        empty_target_seq[0, 0] = sampled_word_index\n",
    "        \n",
    "        # Update the decoder states\n",
    "        states_values = [h, c]\n",
    "    \n",
    "    # Output the final translation\n",
    "    print(\"Bot:\" + decoded_translation.replace(' end', ''))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "database",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
